{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/sam2/sam2/modeling/sam/transformer.py:23: UserWarning: Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.\n",
      "  OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 12:10:48.777391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-15 12:10:48.777420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-15 12:10:48.778426: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-15 12:10:48.783907: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 12:10:49.445973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#print(dir(sam2))\n",
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#from sam2.build_sam import build_sam2\n",
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "#from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import sys\n",
    "from config import topview_vec, sideview_vec, tcp_X_offset, tcp_Y_offset, tcp_Z_offset, n_depth_samples, vit_thresh, tower\n",
    "# sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control.ur5 import UR5_Interface as robot\n",
    "from magpie_perception.label_owlv2 import LabelOWLv2\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "from openai import OpenAI\n",
    "\n",
    "from control_scripts import goto_vec, get_pictures, get_frames\n",
    "from gpt_planning import get_gpt_next_instruction, print_json\n",
    "from APIKeys import API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting robot from nb\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could NOT connect to gripper Dynamixel board!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m myrobot \u001b[38;5;241m=\u001b[39m robot()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting robot from nb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmyrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m label_vit \u001b[38;5;241m=\u001b[39m LabelOWLv2(topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, score_threshold\u001b[38;5;241m=\u001b[39mvit_thresh, cpu_override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m label_vit\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/magpie_control/ur5.py:121\u001b[0m, in \u001b[0;36mUR5_Interface.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctrl \u001b[38;5;241m=\u001b[39m rtde_control\u001b[38;5;241m.\u001b[39mRTDEControlInterface( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobotIP )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecv \u001b[38;5;241m=\u001b[39m rtde_receive\u001b[38;5;241m.\u001b[39mRTDEReceiveInterface( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobotIP, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq )\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_gripper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/magpie_control/ur5.py:105\u001b[0m, in \u001b[0;36mUR5_Interface.start_gripper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgripper\u001b[38;5;241m.\u001b[39mset_torque( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorqLim, finger\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould NOT connect to gripper Dynamixel board!\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could NOT connect to gripper Dynamixel board!"
     ]
    }
   ],
   "source": [
    "myrs = real.RealSense()\n",
    "myrs.initConnection()\n",
    "myrobot = robot()\n",
    "print(f\"starting robot from nb\")\n",
    "myrobot.start()\n",
    "label_vit = LabelOWLv2(topk=1, score_threshold=vit_thresh, cpu_override=False)\n",
    "label_vit.model.eval()\n",
    "#print(dir(label_vit.model))\n",
    "print(f\"{label_vit.model.device=}\")\n",
    "\n",
    "sam_predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "print(f\"{sam_predictor.model.device=}\")\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key= API_KEY,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrobot.open_gripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_frame_intrinsics(rs_wrapper):\n",
    "    rgb_frame, depth_frame = get_frames(rs_wrapper)\n",
    "    intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "    return depth_frame, intrinsics\n",
    "\n",
    "def get_topview_depth(point_list, UR_interface, rs_wrapper):\n",
    "    goto_vec(UR_interface, topview_vec)\n",
    "    depth_measurements = [[] for p in point_list]\n",
    "    intrinsics = None\n",
    "    for i in range(n_depth_samples):\n",
    "        depth_frame, intrinsics = get_depth_frame_intrinsics(rs_wrapper)\n",
    "        for i, (x, y) in enumerate(point_list):\n",
    "            depth_val = depth_frame.get_distance(x, y)  # in meters\n",
    "            if depth_val > 0:\n",
    "                depth_measurements[i].append(depth_val)\n",
    "    \n",
    "    depth_measurements = [np.array(point_measurements) for point_measurements in depth_measurements]\n",
    "    final_depth_measurements = [0 for point in point_list]\n",
    "    for i, measurements in enumerate(depth_measurements):\n",
    "        std = np.std(measurements)\n",
    "        mean = np.mean(measurements)\n",
    "        in_std_mask = np.abs(measurements-mean) <= std\n",
    "        depth_measurements = measurements[in_std_mask]\n",
    "        #print(f\"{depth_measurements=}\")\n",
    "        depth_val = sum(measurements)/len(measurements)\n",
    "        #print(f\"{depth_val=}\")\n",
    "        assert depth_val > 0, f\"not able to get depth val after {n_depth_samples} samples {depth_val=}\"\n",
    "        final_depth_measurements[i] = depth_val\n",
    "    return final_depth_measurements\n",
    "\n",
    "def deproject_top_view_point(K, pixel_x, pixel_y, depth):\n",
    "    return rs.rs2_deproject_pixel_to_point(K, [pixel_x, pixel_y], depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "pick_place_BB takes the color of the block that needs to be picked up and the color of the block \n",
    "to be placed on and gives the cords of where each are located as {xmin, ymin, xmax, ymax, strlabel}\n",
    "\n",
    "\"\"\"\n",
    "def get_bb_patch(bb, edge_color = \"r\"):\n",
    "    rect = patches.Rectangle(\n",
    "                            (bb[\"xmin\"], bb[\"ymin\"]),\n",
    "                                bb[\"xmax\"] - bb[\"xmin\"],\n",
    "                                bb[\"ymax\"] - bb[\"ymin\"],\n",
    "                                linewidth=2, edgecolor=edge_color, facecolor='none'\n",
    "                            )\n",
    "    return rect\n",
    "\n",
    "def topview_pick_place_BB(UR_Interface, rs_wrapper, pick_str, place_str, display= False):\n",
    "    #print(place_str)\n",
    "    #initlize local variables\n",
    "    goto_vec(UR_Interface, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(rs_wrapper)\n",
    "    pick_cord_dict, place_cord_dict = {} , {}\n",
    "    str_cords = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "    queries = [pick_str, place_str]\n",
    "    abbrevq = [pick_str, place_str]\n",
    "    \n",
    "    #pick bboxes and cords\n",
    "    with torch.no_grad():\n",
    "        pick_bbox = label_vit.label(rgb_img, queries[0], abbrevq[0], plot=False, topk=True)\n",
    "        place_bbox = label_vit.label(rgb_img, queries[1], abbrevq[1], plot=False, topk=True)\n",
    "\n",
    "    pick_bbox_cords = pick_bbox[1][0].tolist()\n",
    "    for pick, cord in zip(pick_bbox_cords, str_cords):\n",
    "        #print(f\"{pick=}\")\n",
    "        pick_cord_dict[cord] = int(pick)\n",
    "    \n",
    "    #place bboxes and cords\n",
    "    place_bbox_cords = place_bbox[1][0].tolist()\n",
    "    for place , cord in zip(place_bbox_cords, str_cords):\n",
    "        place_cord_dict[cord] = int(place)\n",
    "    \n",
    "    invalid_border_px_x = 200\n",
    "    invalid_border_px_y = 20\n",
    "    print(f\"{rgb_img.shape=}\")\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    for k in [\"xmin\", \"xmax\"]:\n",
    "        pick_cord_dict[k] = np.clip(pick_cord_dict[k], 0+invalid_border_px_x, rgb_img.shape[1]-invalid_border_px_x)\n",
    "        place_cord_dict[k] = np.clip(place_cord_dict[k], 0+invalid_border_px_x, rgb_img.shape[1]-invalid_border_px_x)\n",
    "    for k in [\"ymin\", \"ymax\"]:\n",
    "        pick_cord_dict[k] = np.clip(pick_cord_dict[k], 0+invalid_border_px_y, rgb_img.shape[0]-invalid_border_px_y)\n",
    "        place_cord_dict[k] = np.clip(place_cord_dict[k], 0+invalid_border_px_y, rgb_img.shape[0]-invalid_border_px_y)\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    pick_cord_dict[\"strlabel\"] = pick_str\n",
    "    place_cord_dict[\"strlabel\"] = place_str\n",
    "\n",
    "    pick_cord_dict[\"xCenter\"] = int((pick_cord_dict['xmin'] + pick_cord_dict[\"xmax\"])/2)\n",
    "    pick_cord_dict[\"yCenter\"] = int((pick_cord_dict['ymin'] + pick_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    place_cord_dict[\"xCenter\"] = int((place_cord_dict['xmin'] + place_cord_dict[\"xmax\"])/2)\n",
    "    place_cord_dict[\"yCenter\"] = int((place_cord_dict['ymin'] + place_cord_dict[\"ymax\"])/2)\n",
    "    \n",
    "    #print(f\"{pick_cord_dict=}\")\n",
    "    #print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    if display:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "        axes[0, 0].imshow(rgb_img)\n",
    "        axes[1, 0].imshow(depth_img)\n",
    "        \n",
    "        axes[0, 0].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "        axes[1, 0].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "\n",
    "        axes[0, 0].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1, 0].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "\n",
    "        axes[0, 0].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "        axes[1, 0].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "\n",
    "        axes[0, 0].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1, 0].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        \n",
    "        axes[0, 0].set_title(\"pick and place bb in rgb\", fontsize=14)\n",
    "        axes[1, 0].set_title(\"pick and place bb in depth\", fontsize=14)\n",
    "\n",
    "        sam_predictor.set_image(rgb_img)\n",
    "        \n",
    "        input_box = np.array([pick_cord_dict[\"xmin\"],  pick_cord_dict[\"ymin\"],  pick_cord_dict[\"xmax\"],  pick_cord_dict[\"ymax\"]])\n",
    "        pick_mask, pick_scores, pick_logits = sam_predictor.predict(box=input_box)\n",
    "        pick_mask = np.transpose(pick_mask, (1, 2, 0))\n",
    "        axes[0, 1].set_title(\"pick segmentation\", fontsize=14)\n",
    "        axes[0, 1].imshow(pick_mask)\n",
    "\n",
    "        input_box = np.array([place_cord_dict[\"xmin\"],  place_cord_dict[\"ymin\"],  place_cord_dict[\"xmax\"],  place_cord_dict[\"ymax\"]])\n",
    "        place_mask, place_scores, place_logits = sam_predictor.predict(box=input_box)\n",
    "        place_mask = np.transpose(place_mask, (1, 2, 0))\n",
    "        axes[1, 1].set_title(\"place segmentation\", fontsize=14)\n",
    "        axes[1, 1].imshow(place_mask)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    _, K = get_depth_frame_intrinsics(rs_wrapper)\n",
    "    depth_querry_list = [\n",
    "        (pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"]),\n",
    "        (pick_cord_dict['xmin'], pick_cord_dict['ymin']),\n",
    "        (pick_cord_dict['xmax'], pick_cord_dict['ymin']),\n",
    "        (place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"]),\n",
    "        (place_cord_dict['xmin'], place_cord_dict['ymin']),\n",
    "        (place_cord_dict['xmax'], place_cord_dict['ymin'])\n",
    "    ]\n",
    "    pick_c_d, pick_ll_d, pick_lr_d, place_c_d, place_ll_d, place_lr_d = get_topview_depth(depth_querry_list, UR_Interface, rs_wrapper)\n",
    "\n",
    "    pick_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"], pick_c_d)\n",
    "    pick_LL_X, pick_LL_Y, pick_LL_Z = deproject_top_view_point(K, pick_cord_dict['xmin'], pick_cord_dict['ymin'], pick_ll_d)\n",
    "    pick_LR_X, pick_LR_Y, pick_LR_Z = deproject_top_view_point(K, pick_cord_dict['xmax'], pick_cord_dict['ymin'], pick_lr_d)\n",
    "    pick_sidelength =  (pick_LL_X-pick_LR_X)**2\n",
    "    pick_sidelength += (pick_LL_Y-pick_LR_Y)**2\n",
    "    pick_sidelength += (pick_LL_Z-pick_LR_Z)**2\n",
    "    pick_sidelength = np.sqrt(pick_sidelength)\n",
    "    pick_cord_dict[\"sidelength\"] = pick_sidelength\n",
    "    pick_cord_dict[\"sidelength\"] = 0.04\n",
    "\n",
    "    print(f\"{pick_cord_dict=}\")\n",
    "\n",
    "    place_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"], place_c_d)\n",
    "    place_LL_X, place_LL_Y, place_LL_Z = deproject_top_view_point(K, place_cord_dict['xmin'], place_cord_dict['ymin'], place_ll_d)\n",
    "    place_LR_X, place_LR_Y, place_LR_Z = deproject_top_view_point(K, place_cord_dict['xmax'], place_cord_dict['ymin'], place_lr_d)\n",
    "    place_sidelength =  (place_LL_X-place_LR_X)**2\n",
    "    place_sidelength += (place_LL_Y-place_LR_Y)**2\n",
    "    place_sidelength += (place_LL_Z-place_LR_Z)**2\n",
    "    place_sidelength = np.sqrt(place_sidelength)\n",
    "    #place_cord_dict[\"sidelength\"] = place_sidelength\n",
    "    pick_cord_dict[\"sidelength\"] = 0.04\n",
    "\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #return dictionarys with {xmin, ymin, xmax, ymax, strlabel}\n",
    "    #return (pick_bbox, pick_cord_dict) , (place_bbox, place_cord_dict)\n",
    "    return pick_cord_dict, place_cord_dict\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset - bb[\"sidelength\"]\n",
    "    \n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    #print(goal_vec)\n",
    "    #UR_interface.disconnect()\n",
    "    #input()\n",
    "\n",
    "    UR_interface.close_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    return success\n",
    "\n",
    "#pick(myrobot, myrs, pick_bb, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset\n",
    "    #if \"table\" in bb[\"strlabel\"]:\n",
    "    #    new_X += random.uniform(-0.05, 0.05)\n",
    "    #    new_Y += random.uniform(0, 0.05)\n",
    "\n",
    "    \n",
    "\n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "\n",
    "    UR_interface.open_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#myrobot.stop()\n",
    "#myrs.disconnect()\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\", \"orange block\"]\n",
    "tower = [ \"yellow block\", \"blue block\", \"green block\"]#, \"green block\"]\n",
    "#tower = [\"yellow block\", \"red block\"]\n",
    "for i in range(0, len(tower)-1):\n",
    "     pick_str = tower[i+1]\n",
    "     place_str = tower[i]\n",
    "     pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "     pick(myrobot, pick_bb)\n",
    "     place(myrobot, place_bb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_directory(base_dir):\n",
    "    # List all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    # Extract the numeric part of the directory names (e.g., 'run_0', 'run_1')\n",
    "    subdirectories = [d for d in subdirectories if d.startswith('run_')]\n",
    "    subdirectories = [int(d.split('_')[1]) for d in subdirectories if d.split('_')[1].isdigit()]\n",
    "    \n",
    "    # If there are no subdirectories, start from 0, else find the max and add 1\n",
    "    next_dir = max(subdirectories, default=-1) + 1\n",
    "\n",
    "    # Create the new directory with the name 'run_<next_dir>'\n",
    "    next_dir_path = os.path.join(base_dir, f\"run_{next_dir}\")\n",
    "    os.makedirs(next_dir_path, exist_ok=True)\n",
    "    \n",
    "    return next_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\n",
    "\n",
    "n_attempts = 2*len(tower)\n",
    "Done = 0\n",
    "i = 0\n",
    "top_dir = \"./data_collection/\"\n",
    "os.makedirs(top_dir, exist_ok=True)\n",
    "save_dir = create_next_directory(top_dir)\n",
    "action_history = []\n",
    "previous_plan = []\n",
    "while(not Done and i < n_attempts):\n",
    "    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\n",
    "    os.makedirs(interation_output_dir, exist_ok=True)\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"side view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\n",
    "\n",
    "    (state_resp, state, state_sys_prompt, state_usr_promot), (instruction_resp, next_instruction, future_instructions, instruction_sys_prompt, instruction_usr_prompt) = get_gpt_next_instruction(client, rgb_img, tower, action_history, previous_plan)\n",
    "    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\n",
    "        state_str = print_json(state, name=\"state\")\n",
    "        file.write(f\"{state_str}\\n\")\n",
    "\n",
    "        next_instruction_str = print_json(next_instruction, \"next instruction\")\n",
    "        file.write(f\"{next_instruction_str}\")\n",
    "\n",
    "\n",
    "        instruction_plan_str = print_json(future_instructions, name=\"plan\")\n",
    "        file.write(f\"{instruction_plan_str}\")\n",
    "\n",
    "        file.write(f\"{state_sys_prompt=}\\n\")\n",
    "        file.write(f\"{state_usr_promot=}\\n\")\n",
    "        file.write(f\"{instruction_sys_prompt=}\\n\")\n",
    "        file.write(f\"{instruction_usr_prompt=}\\n\")\n",
    "        print(f\"{instruction_usr_prompt=}\")\n",
    "    \n",
    "    action_history.append(next_instruction)\n",
    "    previous_plan = future_instructions\n",
    "    Done = int(next_instruction[\"done\"])\n",
    "    \n",
    "   \n",
    "\n",
    "    if Done:\n",
    "        break\n",
    "    pick_str= next_instruction['pick']\n",
    "    place_str= next_instruction['place']\n",
    "    goto_vec(myrobot, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"top view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\n",
    "    pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "    pick(myrobot, pick_bb)\n",
    "    place(myrobot, place_bb)\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
