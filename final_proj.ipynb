{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from config import topview_vec, sideview_vec, tcp_X_offset, tcp_Y_offset, tcp_Z_offset, n_depth_samples, arm_speed, vit_thresh\n",
    "# sys.path.append(\"../\")\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control.ur5 import UR5_Interface as robot\n",
    "from magpie_perception.label_owlv2 import LabelOWLv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrs = real.RealSense()\n",
    "myrs.initConnection()\n",
    "myrobot = robot()\n",
    "myrobot.start()\n",
    "label_vit = LabelOWLv2(topk=1, score_threshold=vit_thresh, cpu_override=False)\n",
    "label_vit.model.eval()\n",
    "#print(dir(label_vit.model))\n",
    "print(label_vit.model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrobot.open_gripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(rsWrapper):\n",
    "    pipe, config = rsWrapper.pipe, rsWrapper.config\n",
    "    frames = pipe.wait_for_frames()\n",
    "    #alignOperator = rs.align(rs.stream.color)\n",
    "    #alignOperator.process(frames)\n",
    "    depthFrame = frames.get_depth_frame()  # pyrealsense2.depth_frame\n",
    "    colorFrame = frames.get_color_frame()\n",
    "    return colorFrame, depthFrame\n",
    "def get_pictures(rsWrapper):\n",
    "    colorFrame, depthFrame = get_frames(rsWrapper)\n",
    "    #print(f\"{type(starting_img)=}\")\n",
    "    #print(f\"{dir(starting_img)=}\")\n",
    "    color_image = np.asarray(colorFrame.get_data())\n",
    "    #color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    depth_image = np.asarray(depthFrame.get_data())\n",
    "    return color_image, depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_vector_distance(goal_vec, actual_pose):\n",
    "    \"\"\"\n",
    "    Check if two 6-degree pose vectors are equivalent within a specified tolerance.\n",
    "\n",
    "    Parameters:\n",
    "    - goal_vec: List of target pose [x, y, z, rx, ry, rz] in meters.\n",
    "    - actual_pose: List of actual pose [x, y, z, rx, ry, rz] in meters.\n",
    "    - tolerance_cm: Tolerance in centimeters (default is 0.01 meters).\n",
    "\n",
    "    Returns:\n",
    "    - True if the poses are equivalent within the tolerance, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Calculate linear distance\n",
    "    linear_distance = np.linalg.norm(np.array(goal_vec[:3]) - np.array(actual_pose[:3]))\n",
    "\n",
    "    # Calculate angular differences (wrap around at 2*pi)\n",
    "    angular_diffs = [\n",
    "        np.arctan2(np.sin(goal_vec[i] - actual_pose[i]), np.cos(goal_vec[i] - actual_pose[i]))\n",
    "        for i in range(3, 6)\n",
    "    ]\n",
    "    angular_distance = np.linalg.norm(angular_diffs)\n",
    "\n",
    "    # Total distance check (considering both linear and angular)\n",
    "    return linear_distance, angular_distance\n",
    "\n",
    "def goto_vec(UR_interface, goal_vec, warning_tolorance=0.01, failure_tolerance=0.1):\n",
    "    #print(f\"{goal_vec=}\")\n",
    "    goal_matrix = UR_interface.poseVectorToMatrix(goal_vec)\n",
    "    UR_interface.moveL(goal_matrix, linSpeed=arm_speed, asynch=False)\n",
    "    actual_pose = UR_interface.recv.getActualTCPPose()\n",
    "    #print(f\"{actual_pose=}\")\n",
    "    linear_error, angular_error = pose_vector_distance(goal_vec, actual_pose)\n",
    "    \n",
    "    success = True\n",
    "    if linear_error >= warning_tolorance:\n",
    "        assert linear_error < failure_tolerance, f\"Linear Error greater than failure tolerance {linear_error=} {goal_vec=} {actual_pose=}\"\n",
    "        warnings.warn(f\"Linear Error greater than warning tolerance {linear_error=} {goal_vec=} {actual_pose=}\")\n",
    "        success = False\n",
    "    if angular_error >= warning_tolorance:\n",
    "        assert angular_error < failure_tolerance, f\"Angular Error greater than failure tolerance {linear_error=} {goal_vec=} {actual_pose=}\"\n",
    "        warnings.warn(f\"Angular Error greater than warning tolerance {angular_error=} {goal_vec=} {actual_pose=}\")\n",
    "        success = False\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "from APIKeys import API_KEY\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= API_KEY,\n",
    ")\n",
    "\n",
    "#Function that takes a list of blocks in order of the tower [red, green, blue] and produces a prompt to be given to the GPT4o in sideview\n",
    "#https://platform.openai.com/docs/guides/structured-outputs/examples\n",
    "def get_state_querry_prompt():\n",
    "    system_prompt = (\"\"\"\n",
    "You are a block stacking robot, your job is to take an image\n",
    "Then you should output a json output with fields\n",
    "objects:list of objects in the scene relevent to the block stacking task\n",
    "object_relationships list of triples of objects and strings describing the relationships between objects\n",
    "                \"\"\")\n",
    "    user_prompt = f\"Your current task is to give me the state in the given image\"\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "def get_instruction_prompt(str_list_stack_order, state_obj):\n",
    "    system_prompt = (\"\"\"\n",
    "You are a block stacking robot, your job is to take an order of colored blocks to build a tower\n",
    "and a state description of the current state of the blocks\n",
    "Then you should output a json output with fields\n",
    "pick:object to be picked up,\n",
    "place:object to place the pick object on top of,\n",
    "Done:0/1 when the tower is complete,\n",
    "explanation: which explains why this a good move to get to our desired tower,\n",
    "only output done if pick is None and place is None\n",
    "                \"\"\")\n",
    "    user_prompt = f\"\"\"Your current task is to give me the next instruction so the blocks are stacked from bottom to top in order {str_list_stack_order}\n",
    "      for objects {state_obj[\"objects\"]} with relationships {state_obj[\"object_relationships\"]}\"\"\"\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "#helper function that formats the image for GPT api\n",
    "def ndarray_to_base64(ndarray):\n",
    "    # Convert the ndarray to a PIL Image\n",
    "    image = Image.fromarray(ndarray)\n",
    "    \n",
    "    # Create a BytesIO object to save the image\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")  # Specify the format you want\n",
    "    # Get the byte data and encode to base64\n",
    "    encoded_string = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "    return encoded_string\n",
    "\n",
    "#api calling function\n",
    "def get_gpt_next_instruction(rgb_image, desired_tower_order):\n",
    "    image = ndarray_to_base64(rgb_image)\n",
    "    img_type = \"image/jpeg\"\n",
    "\n",
    "    state_querry_system_prompt, state_querry_user_prompt = get_state_querry_prompt()\n",
    "    state_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": state_querry_system_prompt},  # Only text in the system message\n",
    "            { \"role\": \"user\", \"content\": state_querry_user_prompt,\n",
    "                \"attachments\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": f\"data:{img_type};base64,{image}\"}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    state_json = json.loads(state_response.choices[0].message.content)\n",
    "    instruction_system_prompt, instruction_user_prompt = get_instruction_prompt(desired_tower_order, state_json)\n",
    "\n",
    "    instruction_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": instruction_system_prompt},  # Only text in the system message\n",
    "            { \"role\": \"user\", \"content\": instruction_user_prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    instruction_json = json.loads(instruction_response.choices[0].message.content)\n",
    "    return (state_response, state_json), (instruction_response, instruction_json)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "goto_vec(myrobot, sideview_vec)\n",
    "rgb_img, depth_img = get_pictures(myrs)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(rgb_img)\n",
    "plt.show()\n",
    "plt.imsave(\"./test_img.jpg\", rgb_img)\n",
    "\n",
    "##--string for GPT QUERY--##\n",
    "(state_response, state_json), (instruction_response, instruction_json) = get_gpt_next_instruction(rgb_img, [\"red block\", \"blue block\", \"yellow block\"])\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"{state_json['objects']=}\")\n",
    "print(f\"{state_json['object_relationships']=}\")\n",
    "\n",
    "print(f\"{instruction_json['pick']=}\")\n",
    "\n",
    "print(f\"{instruction_json['place']=}\")\n",
    "\n",
    "print(f\"{instruction_json['Done']=}\")\n",
    "\n",
    "print(f\"{instruction_json['explanation']=}\")\n",
    "print()\n",
    "print(f\"{state_response=}\")\n",
    "print(f\"{instruction_response=}\")\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_frame_intrinsics(rs_wrapper):\n",
    "    rgb_frame, depth_frame = get_frames(rs_wrapper)\n",
    "    intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "    return depth_frame, intrinsics\n",
    "\n",
    "def get_topview_depth(point_list, UR_interface, rs_wrapper):\n",
    "\n",
    "    goto_vec(UR_interface, topview_vec)\n",
    "    depth_measurements = [[] for p in point_list]\n",
    "    intrinsics = None\n",
    "    for i in range(n_depth_samples):\n",
    "        depth_frame, intrinsics = get_depth_frame_intrinsics(rs_wrapper)\n",
    "        for i, (x, y) in enumerate(point_list):\n",
    "            depth_val = depth_frame.get_distance(x, y)  # in meters\n",
    "            if depth_val > 0:\n",
    "                depth_measurements[i].append(depth_val)\n",
    "    \n",
    "    depth_measurements = [np.array(point_measurements) for point_measurements in depth_measurements]\n",
    "    final_depth_measurements = [0 for point in point_list]\n",
    "    for i, measurements in enumerate(depth_measurements):\n",
    "        std = np.std(measurements)\n",
    "        mean = np.mean(measurements)\n",
    "        in_std_mask = np.abs(measurements-mean) <= std\n",
    "        depth_measurements = measurements[in_std_mask]\n",
    "        #print(f\"{depth_measurements=}\")\n",
    "        depth_val = sum(measurements)/len(measurements)\n",
    "        #print(f\"{depth_val=}\")\n",
    "        assert depth_val > 0, f\"not able to get depth val after {n_depth_samples} samples {depth_val=}\"\n",
    "        final_depth_measurements[i] = depth_val\n",
    "    return final_depth_measurements\n",
    "\n",
    "def deproject_top_view_point(K, pixel_x, pixel_y, depth):\n",
    "    return rs.rs2_deproject_pixel_to_point(K, [pixel_x, pixel_y], depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "pick_place_BB takes the color of the block that needs to be picked up and the color of the block \n",
    "to be placed on and gives the cords of where each are located as {xmin, ymin, xmax, ymax, strlabel}\n",
    "\n",
    "\"\"\"\n",
    "def get_bb_patch(bb, edge_color = \"r\"):\n",
    "    rect = patches.Rectangle(\n",
    "                            (bb[\"xmin\"], bb[\"ymin\"]),\n",
    "                                bb[\"xmax\"] - bb[\"xmin\"],\n",
    "                                bb[\"ymax\"] - bb[\"ymin\"],\n",
    "                                linewidth=2, edgecolor=edge_color, facecolor='none'\n",
    "                            )\n",
    "    return rect\n",
    "\n",
    "def topview_pick_place_BB(UR_Interface, rs_wrapper, pick_str, place_str, display= False):\n",
    "    #print(place_str)\n",
    "    #initlize local variables\n",
    "    goto_vec(UR_Interface, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(rs_wrapper)\n",
    "    pick_cord_dict, place_cord_dict = {} , {}\n",
    "    str_cords = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "    queries = [pick_str, place_str]\n",
    "    abbrevq = [pick_str, place_str]\n",
    "    \n",
    "    #pick bboxes and cords\n",
    "    with torch.no_grad():\n",
    "        pick_bbox = label_vit.label(rgb_img, queries[0], abbrevq[0], plot=False, topk=True)\n",
    "        place_bbox = label_vit.label(rgb_img, queries[1], abbrevq[1], plot=False, topk=True)\n",
    "\n",
    "    pick_bbox_cords = pick_bbox[1][0].tolist()\n",
    "    for pick, cord in zip(pick_bbox_cords, str_cords):\n",
    "        #print(f\"{pick=}\")\n",
    "        pick_cord_dict[cord] = int(pick)\n",
    "    \n",
    "    #place bboxes and cords\n",
    "    place_bbox_cords = place_bbox[1][0].tolist()\n",
    "    for place , cord in zip(place_bbox_cords, str_cords):\n",
    "        place_cord_dict[cord] = int(place)\n",
    "    \n",
    "    invalid_border_px_x = 200\n",
    "    invalid_border_px_y = 20\n",
    "    print(f\"{rgb_img.shape=}\")\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    for k in [\"xmin\", \"xmax\"]:\n",
    "        pick_cord_dict[k] = np.clip(pick_cord_dict[k], 0+invalid_border_px_x, rgb_img.shape[1]-invalid_border_px_x)\n",
    "        place_cord_dict[k] = np.clip(place_cord_dict[k], 0+invalid_border_px_x, rgb_img.shape[1]-invalid_border_px_x)\n",
    "    for k in [\"ymin\", \"ymax\"]:\n",
    "        pick_cord_dict[k] = np.clip(pick_cord_dict[k], 0+invalid_border_px_y, rgb_img.shape[0]-invalid_border_px_y)\n",
    "        place_cord_dict[k] = np.clip(place_cord_dict[k], 0+invalid_border_px_y, rgb_img.shape[0]-invalid_border_px_y)\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    pick_cord_dict[\"strlabel\"] = pick_str\n",
    "    place_cord_dict[\"strlabel\"] = place_str\n",
    "\n",
    "    pick_cord_dict[\"xCenter\"] = int((pick_cord_dict['xmin'] + pick_cord_dict[\"xmax\"])/2)\n",
    "    pick_cord_dict[\"yCenter\"] = int((pick_cord_dict['ymin'] + pick_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    place_cord_dict[\"xCenter\"] = int((place_cord_dict['xmin'] + place_cord_dict[\"xmax\"])/2)\n",
    "    place_cord_dict[\"yCenter\"] = int((place_cord_dict['ymin'] + place_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    #print(f\"{pick_cord_dict=}\")\n",
    "    #print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    if display:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "        axes[0].imshow(rgb_img)\n",
    "        axes[1].imshow(depth_img)\n",
    "        \n",
    "        axes[0].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "        axes[1].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "\n",
    "        axes[0].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "\n",
    "        axes[0].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "        axes[1].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "\n",
    "        axes[0].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        \n",
    "        axes[0].set_title(\"pick and place bb in rgb\", fontsize=14)\n",
    "        axes[1].set_title(\"pick and place bb in depth\", fontsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    _, K = get_depth_frame_intrinsics(rs_wrapper)\n",
    "    depth_querry_list = [\n",
    "        (pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"]),\n",
    "        (pick_cord_dict['xmin'], pick_cord_dict['ymin']),\n",
    "        (pick_cord_dict['xmax'], pick_cord_dict['ymin']),\n",
    "        (place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"]),\n",
    "        (place_cord_dict['xmin'], place_cord_dict['ymin']),\n",
    "        (place_cord_dict['xmax'], place_cord_dict['ymin'])\n",
    "    ]\n",
    "    pick_c_d, pick_ll_d, pick_lr_d, place_c_d, place_ll_d, place_lr_d = get_topview_depth(depth_querry_list, UR_Interface, rs_wrapper)\n",
    "\n",
    "    pick_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"], pick_c_d)\n",
    "    pick_LL_X, pick_LL_Y, pick_LL_Z = deproject_top_view_point(K, pick_cord_dict['xmin'], pick_cord_dict['ymin'], pick_ll_d)\n",
    "    pick_LR_X, pick_LR_Y, pick_LR_Z = deproject_top_view_point(K, pick_cord_dict['xmax'], pick_cord_dict['ymin'], pick_lr_d)\n",
    "    pick_sidelength =  (pick_LL_X-pick_LR_X)**2\n",
    "    pick_sidelength += (pick_LL_Y-pick_LR_Y)**2\n",
    "    pick_sidelength += (pick_LL_Z-pick_LR_Z)**2\n",
    "    pick_sidelength = np.sqrt(pick_sidelength)\n",
    "    pick_cord_dict[\"sidelength\"] = pick_sidelength\n",
    "    print(f\"{pick_cord_dict=}\")\n",
    "\n",
    "    place_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"], place_c_d)\n",
    "    place_LL_X, place_LL_Y, place_LL_Z = deproject_top_view_point(K, place_cord_dict['xmin'], place_cord_dict['ymin'], place_ll_d)\n",
    "    place_LR_X, place_LR_Y, place_LR_Z = deproject_top_view_point(K, place_cord_dict['xmax'], place_cord_dict['ymin'], place_lr_d)\n",
    "    place_sidelength =  (place_LL_X-place_LR_X)**2\n",
    "    place_sidelength += (place_LL_Y-place_LR_Y)**2\n",
    "    place_sidelength += (place_LL_Z-place_LR_Z)**2\n",
    "    place_sidelength = np.sqrt(place_sidelength)\n",
    "    place_cord_dict[\"sidelength\"] = place_sidelength\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #return dictionarys with {xmin, ymin, xmax, ymax, strlabel}\n",
    "    #return (pick_bbox, pick_cord_dict) , (place_bbox, place_cord_dict)\n",
    "    return pick_cord_dict, place_cord_dict\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset - bb[\"sidelength\"]\n",
    "    \n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    #print(goal_vec)\n",
    "    #UR_interface.disconnect()\n",
    "    #input()\n",
    "\n",
    "    UR_interface.close_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    return success\n",
    "\n",
    "#pick(myrobot, myrs, pick_bb, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset\n",
    "\n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "\n",
    "    UR_interface.open_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    return success\n",
    "#place(myrobot, myrs, place_bb, display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#myrobot.stop()\n",
    "#myrs.disconnect()\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\", \"orange block\"]\n",
    "tower = [\"grey table\", \"yellow block\", \"blue block\", \"red block\"]#, \"green block\"]\n",
    "#tower = [\"yellow block\", \"red block\"]\n",
    "for i in range(0, len(tower)-1):\n",
    "     pick_str = tower[i+1]\n",
    "     place_str = tower[i]\n",
    "     pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "     pick(myrobot, pick_bb)\n",
    "     place(myrobot, place_bb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_directory(base_dir):\n",
    "    # List all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    # Extract the numeric part of the directory names (e.g., 'run_0', 'run_1')\n",
    "    subdirectories = [d for d in subdirectories if d.startswith('run_')]\n",
    "    subdirectories = [int(d.split('_')[1]) for d in subdirectories if d.split('_')[1].isdigit()]\n",
    "    \n",
    "    # If there are no subdirectories, start from 0, else find the max and add 1\n",
    "    next_dir = max(subdirectories, default=-1) + 1\n",
    "\n",
    "    # Create the new directory with the name 'run_<next_dir>'\n",
    "    next_dir_path = os.path.join(base_dir, f\"run_{next_dir}\")\n",
    "    os.makedirs(next_dir_path, exist_ok=True)\n",
    "    \n",
    "    return next_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\n",
    "tower = [\"red block\", \"blue block\", \"yellow block\"]\n",
    "n_attempts = 2*len(tower)\n",
    "Done = 0\n",
    "i = 0\n",
    "save_dir = create_next_directory(\"./data_collection/\")\n",
    "\n",
    "while(not Done and i < n_attempts):\n",
    "    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\n",
    "    os.makedirs(interation_output_dir, exist_ok=True)\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"side view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\n",
    "    (state_resp, state), (instruction_resp, instruction) = get_gpt_next_instruction(rgb_img, tower)\n",
    "    Done = int(instruction[\"Done\"])\n",
    "    \n",
    "    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\n",
    "        print()\n",
    "        print(f\"{state['objects']=}\")\n",
    "        file.write(f\"{state['objects']=}\\n\")\n",
    "\n",
    "        print(f\"{state['object_relationships']=}\")\n",
    "        file.write(f\"{state['object_relationships']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['pick']=}\")\n",
    "        file.write(f\"{instruction['pick']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['place']=}\")\n",
    "        file.write(f\"{instruction['place']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['Done']=}\")\n",
    "        file.write(f\"{instruction['Done']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['explanation']=}\")\n",
    "        file.write(f\"{instruction['explanation']=}\\n\")\n",
    "        print()\n",
    "        file.write(f\"{state_resp=}\")\n",
    "        file.write(f\"{instruction_resp=}\")\n",
    "\n",
    "    if Done:\n",
    "        break\n",
    "    pick_str= instruction['pick']\n",
    "    place_str= instruction['place']\n",
    "    goto_vec(myrobot, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"top view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\n",
    "    pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "    pick(myrobot, pick_bb)\n",
    "    place(myrobot, place_bb)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
