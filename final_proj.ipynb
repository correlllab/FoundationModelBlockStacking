{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 15:02:07.227003: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 15:02:07.227031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 15:02:07.228096: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 15:02:07.233525: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-08 15:02:07.939681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from config import topview_vec, sideview_vec, tcp_X_offset, tcp_Y_offset, tcp_Z_offset, n_depth_samples\n",
    "# sys.path.append(\"../\")\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control.ur5 import UR5_Interface as robot\n",
    "from magpie_perception.label_owlv2 import LabelOWLv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could NOT connect to gripper Dynamixel board!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m myrs\u001b[38;5;241m.\u001b[39minitConnection()\n\u001b[1;32m      3\u001b[0m myrobot \u001b[38;5;241m=\u001b[39m robot()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmyrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m label_vit \u001b[38;5;241m=\u001b[39m LabelOWLv2(topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, score_threshold\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, cpu_override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m label_vit\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/magpie_control/ur5.py:121\u001b[0m, in \u001b[0;36mUR5_Interface.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctrl \u001b[38;5;241m=\u001b[39m rtde_control\u001b[38;5;241m.\u001b[39mRTDEControlInterface( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobotIP )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecv \u001b[38;5;241m=\u001b[39m rtde_receive\u001b[38;5;241m.\u001b[39mRTDEReceiveInterface( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobotIP, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq )\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_gripper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/magpie_control/ur5.py:105\u001b[0m, in \u001b[0;36mUR5_Interface.start_gripper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgripper\u001b[38;5;241m.\u001b[39mset_torque( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorqLim, finger\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould NOT connect to gripper Dynamixel board!\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could NOT connect to gripper Dynamixel board!"
     ]
    }
   ],
   "source": [
    "myrs = real.RealSense()\n",
    "myrs.initConnection()\n",
    "myrobot = robot()\n",
    "myrobot.start()\n",
    "label_vit = LabelOWLv2(topk=1, score_threshold= 0.05, cpu_override=False)\n",
    "label_vit.model.eval()\n",
    "#print(dir(label_vit.model))\n",
    "print(label_vit.model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrobot.open_gripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(rsWrapper):\n",
    "    pipe, config = rsWrapper.pipe, rsWrapper.config\n",
    "    frames = pipe.wait_for_frames()\n",
    "    #alignOperator = rs.align(rs.stream.color)\n",
    "    #alignOperator.process(frames)\n",
    "    depthFrame = frames.get_depth_frame()  # pyrealsense2.depth_frame\n",
    "    colorFrame = frames.get_color_frame()\n",
    "    return colorFrame, depthFrame\n",
    "def get_pictures(rsWrapper):\n",
    "    colorFrame, depthFrame = get_frames(rsWrapper)\n",
    "    #print(f\"{type(starting_img)=}\")\n",
    "    #print(f\"{dir(starting_img)=}\")\n",
    "    color_image = np.asarray(colorFrame.get_data())\n",
    "    #color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    depth_image = np.asarray(depthFrame.get_data())\n",
    "    return color_image, depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_vector_distance(goal_vec, actual_pose):\n",
    "    \"\"\"\n",
    "    Check if two 6-degree pose vectors are equivalent within a specified tolerance.\n",
    "\n",
    "    Parameters:\n",
    "    - goal_vec: List of target pose [x, y, z, rx, ry, rz] in meters.\n",
    "    - actual_pose: List of actual pose [x, y, z, rx, ry, rz] in meters.\n",
    "    - tolerance_cm: Tolerance in centimeters (default is 0.01 meters).\n",
    "\n",
    "    Returns:\n",
    "    - True if the poses are equivalent within the tolerance, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Calculate linear distance\n",
    "    linear_distance = np.linalg.norm(np.array(goal_vec[:3]) - np.array(actual_pose[:3]))\n",
    "\n",
    "    # Calculate angular differences (wrap around at 2*pi)\n",
    "    angular_diffs = [\n",
    "        np.arctan2(np.sin(goal_vec[i] - actual_pose[i]), np.cos(goal_vec[i] - actual_pose[i]))\n",
    "        for i in range(3, 6)\n",
    "    ]\n",
    "    angular_distance = np.linalg.norm(angular_diffs)\n",
    "\n",
    "    # Total distance check (considering both linear and angular)\n",
    "    return linear_distance, angular_distance\n",
    "\n",
    "def goto_vec(UR_interface, goal_vec, warning_tolorance=0.01, failure_tolerance=0.1):\n",
    "    #print(f\"{goal_vec=}\")\n",
    "    goal_matrix = UR_interface.poseVectorToMatrix(goal_vec)\n",
    "    UR_interface.moveL(goal_matrix, linSpeed=0.1, asynch=False)\n",
    "    actual_pose = UR_interface.recv.getActualTCPPose()\n",
    "    #print(f\"{actual_pose=}\")\n",
    "    linear_error, angular_error = pose_vector_distance(goal_vec, actual_pose)\n",
    "    \n",
    "    success = True\n",
    "    if linear_error >= warning_tolorance:\n",
    "        assert linear_error < failure_tolerance, f\"Linear Error greater than failure tolerance {linear_error=} {goal_vec=} {actual_pose=}\"\n",
    "        warnings.warn(f\"Linear Error greater than warning tolerance {linear_error=} {goal_vec=} {actual_pose=}\")\n",
    "        success = False\n",
    "    if angular_error >= warning_tolorance:\n",
    "        assert angular_error < failure_tolerance, f\"Angular Error greater than failure tolerance {linear_error=} {goal_vec=} {actual_pose=}\"\n",
    "        warnings.warn(f\"Angular Error greater than warning tolerance {angular_error=} {goal_vec=} {actual_pose=}\")\n",
    "        success = False\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "from APIKeys import API_KEY\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= API_KEY,\n",
    ")\n",
    "class object(BaseModel):\n",
    "    description: str\n",
    "class object_relationship(BaseModel):\n",
    "    object_1: object\n",
    "    object_2: object\n",
    "    relationship: str\n",
    "class SceneDescription(BaseModel):\n",
    "    objects: list[object]\n",
    "    object_relationships: list[object_relationship]\n",
    "class GPT_Instruction(BaseModel):\n",
    "    state: SceneDescription\n",
    "    pick: str\n",
    "    place: str\n",
    "    Done: bool\n",
    "    explanation:str\n",
    "#Function that takes a list of blocks in order of the tower [red, green, blue] and produces a prompt to be given to the GPT4o in sideview\n",
    "#https://platform.openai.com/docs/guides/structured-outputs/examples\n",
    "def set_prompt(str_list_stack_order):\n",
    "    system_prompt = (\"\"\"\n",
    "                     You are a block stacking robot, your job is to take an order of colored blocks to build a tower and an image of the current state of blocks\n",
    "                     Then you should output a strucutured output \n",
    "                     pick:object to be picked up,\n",
    "                    place:object to place the pick object on top of,\n",
    "                    Done:0/1 when the tower is complete,\n",
    "                    state: which describes the state of the blocks in the image and their positions relative to eachother and the table,\n",
    "                    explanation: which explains why this a good move to get to our desired tower,\n",
    "                    dont forget object can be on the table, also objects do not have to be related,\n",
    "                      if you want to place something on the table you must specify \"white paper\"\n",
    "                     only output done if pick is None and place is None\n",
    "                \"\"\")\n",
    "    user_prompt = f\"Your current task is to give me the next instruction so the blocks are stacked from bottom to top in order {str_list_stack_order} from the state in the provided image\"\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "#helper function that formats the image for GPT api\n",
    "def ndarray_to_base64(ndarray):\n",
    "    # Convert the ndarray to a PIL Image\n",
    "    image = Image.fromarray(ndarray)\n",
    "    \n",
    "    # Create a BytesIO object to save the image\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")  # Specify the format you want\n",
    "    # Get the byte data and encode to base64\n",
    "    encoded_string = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "    return encoded_string\n",
    "\n",
    "#api calling function\n",
    "def get_response(system_prompt, user_prompt, img_prompt):\n",
    "    image = ndarray_to_base64(img_prompt)\n",
    "    img_type = \"image/jpeg\"\n",
    "    # Save the image to a file\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        #model=\"gpt-4o-mini\",\n",
    "        model=\"gpt-4o\",\n",
    "        \n",
    "        messages=[\n",
    "            #{ \"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": str_prompt},{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{img_type};base64,{image}\"},},],},\n",
    "            #{\"role\": \"user\", \"content\": user_prompt}\n",
    "\n",
    "            #changed some stuff here commented original version -max\n",
    "            { \"role\": \"system\", \"content\": system_prompt},  # Only text in the system message\n",
    "        \n",
    "            # The user message contains the text prompt and the image URL\n",
    "            { \n",
    "                \"role\": \"user\", \n",
    "                \"content\": user_prompt,\n",
    "                \"attachments\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": f\"data:{img_type};base64,{image}\"}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        response_format=GPT_Instruction\n",
    "    ) \n",
    "    return response, response.choices[0].message.parsed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goto_vec(myrobot, sideview_vec)\n",
    "rgb_img, depth_img = get_pictures(myrs)\n",
    "plt.figure()\n",
    "plt.imshow(rgb_img)\n",
    "plt.show()\n",
    "plt.imsave(\"side_view_0.png\", rgb_img)\n",
    "\n",
    "##--string for GPT QUERY--##\n",
    "sys_prompt, usr_prompt = set_prompt([\"red block\", \"blue block\", \"green block\"])\n",
    "print(f\"{sys_prompt=}\")\n",
    "print(f\"{usr_prompt=}\")\n",
    "\n",
    "##--GPT QUERY--##\n",
    "\"\"\"\n",
    "gptresp, instruction = get_response(sys_prompt, usr_prompt, rgb_img)\n",
    "print(gptresp)\n",
    "print(instruction)\n",
    "print(type(instruction))\n",
    "print(dir(instruction))\n",
    "print(instruction.pick)\n",
    "print(instruction.place)\n",
    "print(instruction.Done)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gpt(response):\n",
    "    content = response[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_frame_intrinsics(rs_wrapper):\n",
    "    rgb_frame, depth_frame = get_frames(rs_wrapper)\n",
    "    intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "    return depth_frame, intrinsics\n",
    "\n",
    "def get_topview_depth(point_list, UR_interface, rs_wrapper):\n",
    "\n",
    "    goto_vec(UR_interface, topview_vec)\n",
    "    depth_measurements = [[] for p in point_list]\n",
    "    intrinsics = None\n",
    "    for i in range(n_depth_samples):\n",
    "        depth_frame, intrinsics = get_depth_frame_intrinsics(rs_wrapper)\n",
    "        for i, (x, y) in enumerate(point_list):\n",
    "            x = min(max(0,x), depth_frame.width-1)\n",
    "            y = min(max(0,y), depth_frame.height-1)\n",
    "            depth_val = depth_frame.get_distance(x, y)  # in meters\n",
    "            if depth_val > 0:\n",
    "                depth_measurements[i].append(depth_val)\n",
    "\n",
    "    \n",
    "    depth_measurements = [np.array(point_measurements) for point_measurements in depth_measurements]\n",
    "    final_depth_measurements = [0 for point in point_list]\n",
    "    for i, measurements in enumerate(depth_measurements):\n",
    "        std = np.std(measurements)\n",
    "        mean = np.mean(measurements)\n",
    "        in_std_mask = np.abs(measurements-mean) <= std\n",
    "        depth_measurements = measurements[in_std_mask]\n",
    "        #print(f\"{depth_measurements=}\")\n",
    "        depth_val = sum(measurements)/len(measurements)\n",
    "        #print(f\"{depth_val=}\")\n",
    "        assert depth_val > 0, f\"not able to get depth val after {n_samples} samples {depth_val=}\"\n",
    "        final_depth_measurements[i] = depth_val\n",
    "    return final_depth_measurements\n",
    "\n",
    "def deproject_top_view_point(K, pixel_x, pixel_y, depth):\n",
    "    return rs.rs2_deproject_pixel_to_point(K, [pixel_x, pixel_y], depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "pick_place_BB takes the color of the block that needs to be picked up and the color of the block \n",
    "to be placed on and gives the cords of where each are located as {xmin, ymin, xmax, ymax, strlabel}\n",
    "\n",
    "\"\"\"\n",
    "def get_bb_patch(bb, edge_color = \"r\"):\n",
    "    rect = patches.Rectangle(\n",
    "                            (bb[\"xmin\"], bb[\"ymin\"]),\n",
    "                                bb[\"xmax\"] - bb[\"xmin\"],\n",
    "                                bb[\"ymax\"] - bb[\"ymin\"],\n",
    "                                linewidth=2, edgecolor=edge_color, facecolor='none'\n",
    "                            )\n",
    "    return rect\n",
    "\n",
    "def topview_pick_place_BB(UR_Interface, rs_wrapper, pick_str, place_str, display= False):\n",
    "    #print(place_str)\n",
    "    #initlize local variables\n",
    "    goto_vec(UR_Interface, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(rs_wrapper)\n",
    "    pick_cord_dict, place_cord_dict = {} , {}\n",
    "    str_cords = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "    queries = [pick_str, place_str]\n",
    "    abbrevq = [pick_str, place_str]\n",
    "    \n",
    "    #pick bboxes and cords\n",
    "    with torch.no_grad():\n",
    "        pick_bbox = label_vit.label(rgb_img, queries[0], abbrevq[0], plot=False, topk=True)\n",
    "        place_bbox = label_vit.label(rgb_img, queries[1], abbrevq[1], plot=False, topk=True)\n",
    "\n",
    "    pick_bbox_cords = pick_bbox[1][0].tolist()\n",
    "    for pick , cord in zip(pick_bbox_cords, str_cords):\n",
    "        pick_cord_dict[cord] = int(pick)\n",
    "    pick_cord_dict[\"strlabel\"] = pick_str\n",
    "    pick_cord_dict[\"xCenter\"] = int((pick_cord_dict['xmin'] + pick_cord_dict[\"xmax\"])/2)\n",
    "    pick_cord_dict[\"yCenter\"] = int((pick_cord_dict['ymin'] + pick_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    \n",
    "    #place bboxes and cords\n",
    "    place_bbox_cords = place_bbox[1][0].tolist()\n",
    "    for pick , cord in zip(place_bbox_cords, str_cords):\n",
    "        place_cord_dict[cord] = int(pick)\n",
    "    place_cord_dict[\"strlabel\"] = place_str\n",
    "    place_cord_dict[\"xCenter\"] = int((place_cord_dict['xmin'] + place_cord_dict[\"xmax\"])/2)\n",
    "    place_cord_dict[\"yCenter\"] = int((place_cord_dict['ymin'] + place_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    #print(f\"{pick_cord_dict=}\")\n",
    "    #print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    if display:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "        axes[0].imshow(rgb_img)\n",
    "        axes[1].imshow(depth_img)\n",
    "        \n",
    "        axes[0].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "        axes[1].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "\n",
    "        axes[0].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "\n",
    "        axes[0].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "        axes[1].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "\n",
    "        axes[0].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        \n",
    "        axes[0].set_title(\"pick and place bb in rgb\", fontsize=14)\n",
    "        axes[1].set_title(\"pick and place bb in depth\", fontsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    _, K = get_depth_frame_intrinsics(rs_wrapper)\n",
    "    depth_querry_list = [\n",
    "        (pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"]),\n",
    "        (pick_cord_dict['xmin'], pick_cord_dict['ymin']),\n",
    "        (pick_cord_dict['xmax'], pick_cord_dict['ymin']),\n",
    "        (place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"]),\n",
    "        (place_cord_dict['xmin'], place_cord_dict['ymin']),\n",
    "        (place_cord_dict['xmax'], place_cord_dict['ymin'])\n",
    "    ]\n",
    "    pick_c_d, pick_ll_d, pick_lr_d, place_c_d, place_ll_d, place_lr_d = get_topview_depth(depth_querry_list, UR_Interface, rs_wrapper)\n",
    "\n",
    "    pick_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"], pick_c_d)\n",
    "    pick_LL_X, pick_LL_Y, pick_LL_Z = deproject_top_view_point(K, pick_cord_dict['xmin'], pick_cord_dict['ymin'], pick_ll_d)\n",
    "    pick_LR_X, pick_LR_Y, pick_LR_Z = deproject_top_view_point(K, pick_cord_dict['xmax'], pick_cord_dict['ymin'], pick_lr_d)\n",
    "    pick_sidelength =  (pick_LL_X-pick_LR_X)**2\n",
    "    pick_sidelength += (pick_LL_Y-pick_LR_Y)**2\n",
    "    pick_sidelength += (pick_LL_Z-pick_LR_Z)**2\n",
    "    pick_sidelength = np.sqrt(pick_sidelength)\n",
    "    pick_cord_dict[\"sidelength\"] = pick_sidelength\n",
    "    print(f\"{pick_cord_dict=}\")\n",
    "\n",
    "    place_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"], place_c_d)\n",
    "    place_LL_X, place_LL_Y, place_LL_Z = deproject_top_view_point(K, place_cord_dict['xmin'], place_cord_dict['ymin'], place_ll_d)\n",
    "    place_LR_X, place_LR_Y, place_LR_Z = deproject_top_view_point(K, place_cord_dict['xmax'], place_cord_dict['ymin'], place_lr_d)\n",
    "    place_sidelength =  (place_LL_X-place_LR_X)**2\n",
    "    place_sidelength += (place_LL_Y-place_LR_Y)**2\n",
    "    place_sidelength += (place_LL_Z-place_LR_Z)**2\n",
    "    place_sidelength = np.sqrt(place_sidelength)\n",
    "    place_cord_dict[\"sidelength\"] = place_sidelength\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #return dictionarys with {xmin, ymin, xmax, ymax, strlabel}\n",
    "    #return (pick_bbox, pick_cord_dict) , (place_bbox, place_cord_dict)\n",
    "    return pick_cord_dict, place_cord_dict\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset - bb[\"sidelength\"]\n",
    "    \n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    #print(goal_vec)\n",
    "    #UR_interface.disconnect()\n",
    "    #input()\n",
    "\n",
    "    UR_interface.close_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    return success\n",
    "\n",
    "#pick(myrobot, myrs, pick_bb, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset\n",
    "\n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "\n",
    "    UR_interface.open_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    return success\n",
    "#place(myrobot, myrs, place_bb, display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#myrobot.stop()\n",
    "#myrs.disconnect()\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\", \"orange block\"]\n",
    "tower = [\"yellow block\", \"blue block\", \"red block\"]#, \"green block\"]\n",
    "#tower = [\"yellow block\", \"red block\"]\n",
    "for i in range(0, len(tower)-1):\n",
    "     pick_str = tower[i+1]\n",
    "     place_str = tower[i]\n",
    "     pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "     pick(myrobot, pick_bb)\n",
    "     place(myrobot, place_bb)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_directory(base_dir):\n",
    "    # List all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    # Extract the numeric part of the directory names (e.g., 'run_0', 'run_1')\n",
    "    subdirectories = [d for d in subdirectories if d.startswith('run_')]\n",
    "    subdirectories = [int(d.split('_')[1]) for d in subdirectories if d.split('_')[1].isdigit()]\n",
    "    \n",
    "    # If there are no subdirectories, start from 0, else find the max and add 1\n",
    "    next_dir = max(subdirectories, default=-1) + 1\n",
    "\n",
    "    # Create the new directory with the name 'run_<next_dir>'\n",
    "    next_dir_path = os.path.join(base_dir, f\"run_{next_dir}\")\n",
    "    os.makedirs(next_dir_path, exist_ok=True)\n",
    "    \n",
    "    return next_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\n",
    "tower = [\"red block\", \"blue block\", \"yellow block\"]\n",
    "n_attempts = 2*len(tower)\n",
    "Done = 0\n",
    "i = 0\n",
    "save_dir = create_next_directory(\"./data_collection/\")\n",
    "\n",
    "while(not Done and i < n_attempts):\n",
    "    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\n",
    "    os.makedirs(interation_output_dir, exist_ok=True)\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"side view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\n",
    "    sys_str, usr_str = set_prompt(tower)\n",
    "    gptresp, instruction = get_response(sys_str, usr_str, rgb_img)\n",
    "    Done = int(instruction.Done)#[\"Done\"])\n",
    "    \n",
    "    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\n",
    "        print()\n",
    "        print(f\"{instruction.state.objects=}\")\n",
    "        file.write(f\"{instruction.state.objects=}\\n\")\n",
    "\n",
    "        print(f\"{instruction.state.object_relationships=}\")\n",
    "        file.write(f\"{instruction.state.object_relationships=}\\n\")\n",
    "\n",
    "        print(f\"{instruction.pick=}\")\n",
    "        file.write(f\"{instruction.pick=}\\n\")\n",
    "\n",
    "        print(f\"{instruction.place=}\")\n",
    "        file.write(f\"{instruction.place=}\\n\")\n",
    "\n",
    "        print(f\"{instruction.Done=}\")\n",
    "        file.write(f\"{instruction.Done=}\\n\")\n",
    "\n",
    "        print(f\"{instruction.explanation=}\")\n",
    "        file.write(f\"{instruction.explanation=}\\n\")\n",
    "        print()\n",
    "    if Done:\n",
    "        break\n",
    "    pick_str= instruction.pick#[\"pick\"]\n",
    "    place_str= instruction.place#[\"place\"]\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"top view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\n",
    "    pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "    pick(myrobot, pick_bb)\n",
    "    place(myrobot, place_bb)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
