{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#print(dir(sam2))\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#from sam2.build_sam import build_sam2\n",
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "#from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import sys\n",
    "from config import topview_vec, sideview_vec, tcp_X_offset, tcp_Y_offset, tcp_Z_offset, n_depth_samples, vit_thresh, tower\n",
    "# sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control.ur5 import UR5_Interface as robot\n",
    "from Observation import observation\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "from openai import OpenAI\n",
    "\n",
    "from control_scripts import goto_vec, get_pictures, get_frames\n",
    "from gpt_planning import get_gpt_next_instruction, print_json\n",
    "from APIKeys import API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWL_model_name = \"google/owlvit-base-patch32\" #vit model to use for bounding boxes\n",
    "class OWLv2:\n",
    "    def __init__(self):\n",
    "        self.processor = OwlViTProcessor.from_pretrained(OWL_model_name)\n",
    "        self.model = OwlViTForObjectDetection.from_pretrained(OWL_model_name)\n",
    "\n",
    "        self.model.to(torch.device(\"cuda\")) if torch.cuda.is_available() else None\n",
    "        self.model.to(torch.device(\"mps\")) if torch.backends.mps.is_available() else None\n",
    "        self.model.eval()  # set model to evaluation mode\n",
    "    def predict(self, img, querries):\n",
    "        \"\"\"\n",
    "        Gets realsense frames\n",
    "        Parameters:\n",
    "        - img: image to produce bounding boxes in\n",
    "        - querries: list of strings whos bounding boxes we want\n",
    "\n",
    "        Returns:\n",
    "        - highest_score_boxes: list of bounding boxes associated with querries\n",
    "        \"\"\"\n",
    "        inputs = self.processor(text=querries, images=img, return_tensors=\"pt\")\n",
    "        inputs.to(torch.device(\"cuda\")) if torch.cuda.is_available() else None\n",
    "        inputs.to(torch.device(\"mps\")) if torch.backends.mps.is_available() else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        target_sizes = torch.tensor([img.shape[:2]])  # (height, width)\n",
    "\n",
    "        results = self.processor.post_process(outputs=outputs, target_sizes=target_sizes)[0]\n",
    "        #print(f\"\\n\\n{results}\\n\\n\")\n",
    "        scores = results[\"scores\"]\n",
    "        labels = results[\"labels\"]\n",
    "        boxes = results[\"boxes\"]\n",
    "        unique_classes = torch.unique(labels)\n",
    "\n",
    "        highest_score_boxes = []\n",
    "\n",
    "        # Find the highest score box for each class\n",
    "        for cls in unique_classes:\n",
    "            # Get indices of the current class\n",
    "            class_indices = (labels == cls).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            # Get scores for the current class\n",
    "            class_scores = scores[class_indices]\n",
    "            \n",
    "            # Find the index of the maximum score\n",
    "            max_index = class_indices[torch.argmax(class_scores)]\n",
    "            \n",
    "            # Get the corresponding box and score\n",
    "            highest_score_boxes.append((querries[int(cls)], boxes[max_index].tolist()))\n",
    "        return highest_score_boxes\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"OWLv2: {self.model.device}\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "label_vit = OWLv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrs = real.RealSense()\n",
    "myrs.initConnection()\n",
    "myrobot = robot()\n",
    "print(f\"starting robot from nb\")\n",
    "myrobot.start()\n",
    "\n",
    "\n",
    "#print(dir(label_vit.model))\n",
    "print(f\"{label_vit.model.device=}\")\n",
    "\n",
    "sam_predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "print(f\"{sam_predictor.model.device=}\")\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key= API_KEY,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrobot.open_gripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topview_pick_place_Observations(UR_Interface, rs_wrapper, pick_str, place_str, display= False):\n",
    "    #print(place_str)\n",
    "    #initlize local variables\n",
    "    goto_vec(UR_Interface, topview_vec)\n",
    "    pick_obs, place_obs = observation(str_label = pick_str) , observation(str_label=place_str)\n",
    "    pick_obs.update_observation(rs_wrapper, label_vit, sam_predictor, topview_vec, display=True)\n",
    "    place_obs.update_observation(rs_wrapper, label_vit, sam_predictor, topview_vec, display=True)\n",
    "    return pick_obs, place_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(UR_interface, obs):\n",
    "    X, Y, Z = obs.ImgFrameWorldCoord\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset - obs.sidelength\n",
    "    \n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    #print(goal_vec)\n",
    "    #UR_interface.disconnect()\n",
    "    #input()\n",
    "\n",
    "    UR_interface.close_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    return success\n",
    "\n",
    "#pick(myrobot, myrs, pick_bb, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place(UR_interface, obs):\n",
    "    X, Y, Z = obs.ImgFrameWorldCoord\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset\n",
    "    #if \"table\" in bb[\"strlabel\"]:\n",
    "    #    new_X += random.uniform(-0.05, 0.05)\n",
    "    #    new_Y += random.uniform(0, 0.05)\n",
    "\n",
    "    \n",
    "\n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "\n",
    "    UR_interface.open_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#myrobot.stop()\n",
    "#myrs.disconnect()\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\", \"orange block\"]\n",
    "temp_tower = [ \"yellow block\", \"blue block\", \"green block\"]#, \"green block\"]\n",
    "#tower = [\"yellow block\", \"red block\"]\n",
    "for i in range(0, len(temp_tower)-1):\n",
    "     pick_str = temp_tower[i+1]\n",
    "     place_str = temp_tower[i]\n",
    "     pick_obs, place_obs = topview_pick_place_Observations(myrobot, myrs, pick_str, place_str, display=True)\n",
    "     pick(myrobot, pick_obs)\n",
    "     place(myrobot, place_obs)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_directory(base_dir):\n",
    "    # List all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    # Extract the numeric part of the directory names (e.g., 'run_0', 'run_1')\n",
    "    subdirectories = [d for d in subdirectories if d.startswith('run_')]\n",
    "    subdirectories = [int(d.split('_')[1]) for d in subdirectories if d.split('_')[1].isdigit()]\n",
    "    \n",
    "    # If there are no subdirectories, start from 0, else find the max and add 1\n",
    "    next_dir = max(subdirectories, default=-1) + 1\n",
    "\n",
    "    # Create the new directory with the name 'run_<next_dir>'\n",
    "    next_dir_path = os.path.join(base_dir, f\"run_{next_dir}\")\n",
    "    os.makedirs(next_dir_path, exist_ok=True)\n",
    "    \n",
    "    return next_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\n",
    "#tower = tower.reverse()\n",
    "n_attempts = 2*len(tower)\n",
    "Done = 0\n",
    "i = 0\n",
    "top_dir = \"./data_collection/\"\n",
    "os.makedirs(top_dir, exist_ok=True)\n",
    "save_dir = create_next_directory(top_dir)\n",
    "action_history = []\n",
    "previous_plan = []\n",
    "while(not Done and i < n_attempts):\n",
    "    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\n",
    "    os.makedirs(interation_output_dir, exist_ok=True)\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"side view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\n",
    "\n",
    "    (state_resp, state, state_sys_prompt, state_usr_promot), (instruction_resp, next_instruction, future_instructions, instruction_sys_prompt, instruction_usr_prompt) = get_gpt_next_instruction(client, rgb_img, tower, action_history, previous_plan)\n",
    "    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\n",
    "        state_str = print_json(state, name=\"state\")\n",
    "        file.write(f\"{state_str}\\n\")\n",
    "\n",
    "        next_instruction_str = print_json(next_instruction, \"next instruction\")\n",
    "        file.write(f\"{next_instruction_str}\")\n",
    "\n",
    "\n",
    "        instruction_plan_str = print_json(future_instructions, name=\"plan\")\n",
    "        file.write(f\"{instruction_plan_str}\")\n",
    "\n",
    "        file.write(f\"{state_sys_prompt=}\\n\")\n",
    "        file.write(f\"{state_usr_promot=}\\n\")\n",
    "        file.write(f\"{instruction_sys_prompt=}\\n\")\n",
    "        file.write(f\"{instruction_usr_prompt=}\\n\")\n",
    "        print(f\"{instruction_usr_prompt=}\")\n",
    "    \n",
    "    action_history.append(next_instruction)\n",
    "    previous_plan = future_instructions\n",
    "    Done = int(next_instruction[\"done\"])\n",
    "    \n",
    "   \n",
    "\n",
    "    if Done:\n",
    "        break\n",
    "    pick_str= next_instruction['pick']\n",
    "    place_str= next_instruction['place']\n",
    "    goto_vec(myrobot, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"top view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\n",
    "    pick_obs, place_obs = topview_pick_place_Observations(myrobot, myrs, pick_str, place_str, display=True)\n",
    "    pick(myrobot, pick_obs)\n",
    "    place(myrobot, place_obs)\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
