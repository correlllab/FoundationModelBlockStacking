{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#print(dir(sam2))\n",
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#from sam2.build_sam import build_sam2\n",
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import torchvision\n",
    "\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from config import topview_vec, sideview_vec, tcp_X_offset, tcp_Y_offset, tcp_Z_offset, n_depth_samples, vit_thresh\n",
    "# sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control.ur5 import UR5_Interface as robot\n",
    "from magpie_perception.label_owlv2 import LabelOWLv2\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from control_scripts import goto_vec, get_pictures, get_frames\n",
    "from gpt_planning import get_gpt_next_instruction\n",
    "from APIKeys import API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrs = real.RealSense()\n",
    "myrs.initConnection()\n",
    "myrobot = robot()\n",
    "myrobot.start()\n",
    "label_vit = LabelOWLv2(topk=1, score_threshold=vit_thresh, cpu_override=False)\n",
    "label_vit.model.eval()\n",
    "#print(dir(label_vit.model))\n",
    "print(f\"{label_vit.model.device=}\")\n",
    "\n",
    "sam_predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "print(f\"{sam_predictor.model.device=}\")\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key= API_KEY,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrobot.open_gripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_frame_intrinsics(rs_wrapper):\n",
    "    rgb_frame, depth_frame = get_frames(rs_wrapper)\n",
    "    intrinsics = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "    return depth_frame, intrinsics\n",
    "\n",
    "def get_topview_depth(point_list, UR_interface, rs_wrapper):\n",
    "    goto_vec(UR_interface, topview_vec)\n",
    "    depth_measurements = [[] for p in point_list]\n",
    "    intrinsics = None\n",
    "    for i in range(n_depth_samples):\n",
    "        depth_frame, intrinsics = get_depth_frame_intrinsics(rs_wrapper)\n",
    "        for i, (x, y) in enumerate(point_list):\n",
    "            depth_val = depth_frame.get_distance(x, y)  # in meters\n",
    "            if depth_val > 0:\n",
    "                depth_measurements[i].append(depth_val)\n",
    "    \n",
    "    depth_measurements = [np.array(point_measurements) for point_measurements in depth_measurements]\n",
    "    final_depth_measurements = [0 for point in point_list]\n",
    "    for i, measurements in enumerate(depth_measurements):\n",
    "        std = np.std(measurements)\n",
    "        mean = np.mean(measurements)\n",
    "        in_std_mask = np.abs(measurements-mean) <= std\n",
    "        depth_measurements = measurements[in_std_mask]\n",
    "        #print(f\"{depth_measurements=}\")\n",
    "        depth_val = sum(measurements)/len(measurements)\n",
    "        #print(f\"{depth_val=}\")\n",
    "        assert depth_val > 0, f\"not able to get depth val after {n_depth_samples} samples {depth_val=}\"\n",
    "        final_depth_measurements[i] = depth_val\n",
    "    return final_depth_measurements\n",
    "\n",
    "def deproject_top_view_point(K, pixel_x, pixel_y, depth):\n",
    "    return rs.rs2_deproject_pixel_to_point(K, [pixel_x, pixel_y], depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "pick_place_BB takes the color of the block that needs to be picked up and the color of the block \n",
    "to be placed on and gives the cords of where each are located as {xmin, ymin, xmax, ymax, strlabel}\n",
    "\n",
    "\"\"\"\n",
    "def get_bb_patch(bb, edge_color = \"r\"):\n",
    "    rect = patches.Rectangle(\n",
    "                            (bb[\"xmin\"], bb[\"ymin\"]),\n",
    "                                bb[\"xmax\"] - bb[\"xmin\"],\n",
    "                                bb[\"ymax\"] - bb[\"ymin\"],\n",
    "                                linewidth=2, edgecolor=edge_color, facecolor='none'\n",
    "                            )\n",
    "    return rect\n",
    "\n",
    "def topview_pick_place_BB(UR_Interface, rs_wrapper, pick_str, place_str, display= False):\n",
    "    #print(place_str)\n",
    "    #initlize local variables\n",
    "    goto_vec(UR_Interface, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(rs_wrapper)\n",
    "    pick_cord_dict, place_cord_dict = {} , {}\n",
    "    str_cords = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "    queries = [pick_str, place_str]\n",
    "    abbrevq = [pick_str, place_str]\n",
    "    \n",
    "    #pick bboxes and cords\n",
    "    with torch.no_grad():\n",
    "        pick_bbox = label_vit.label(rgb_img, queries[0], abbrevq[0], plot=False, topk=True)\n",
    "        place_bbox = label_vit.label(rgb_img, queries[1], abbrevq[1], plot=False, topk=True)\n",
    "\n",
    "    pick_bbox_cords = pick_bbox[1][0].tolist()\n",
    "    for pick, cord in zip(pick_bbox_cords, str_cords):\n",
    "        #print(f\"{pick=}\")\n",
    "        pick_cord_dict[cord] = int(pick)\n",
    "    \n",
    "    #place bboxes and cords\n",
    "    place_bbox_cords = place_bbox[1][0].tolist()\n",
    "    for place , cord in zip(place_bbox_cords, str_cords):\n",
    "        place_cord_dict[cord] = int(place)\n",
    "    \n",
    "    invalid_border_px_x = 200\n",
    "    invalid_border_px_y = 20\n",
    "    print(f\"{rgb_img.shape=}\")\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    for k in [\"xmin\", \"xmax\"]:\n",
    "        pick_cord_dict[k] = np.clip(pick_cord_dict[k], 0+invalid_border_px_x, rgb_img.shape[1]-invalid_border_px_x)\n",
    "        place_cord_dict[k] = np.clip(place_cord_dict[k], 0+invalid_border_px_x, rgb_img.shape[1]-invalid_border_px_x)\n",
    "    for k in [\"ymin\", \"ymax\"]:\n",
    "        pick_cord_dict[k] = np.clip(pick_cord_dict[k], 0+invalid_border_px_y, rgb_img.shape[0]-invalid_border_px_y)\n",
    "        place_cord_dict[k] = np.clip(place_cord_dict[k], 0+invalid_border_px_y, rgb_img.shape[0]-invalid_border_px_y)\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    pick_cord_dict[\"strlabel\"] = pick_str\n",
    "    place_cord_dict[\"strlabel\"] = place_str\n",
    "\n",
    "    pick_cord_dict[\"xCenter\"] = int((pick_cord_dict['xmin'] + pick_cord_dict[\"xmax\"])/2)\n",
    "    pick_cord_dict[\"yCenter\"] = int((pick_cord_dict['ymin'] + pick_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    place_cord_dict[\"xCenter\"] = int((place_cord_dict['xmin'] + place_cord_dict[\"xmax\"])/2)\n",
    "    place_cord_dict[\"yCenter\"] = int((place_cord_dict['ymin'] + place_cord_dict[\"ymax\"])/2)\n",
    "\n",
    "    #print(f\"{pick_cord_dict=}\")\n",
    "    #print(f\"{place_cord_dict=}\")\n",
    "\n",
    "    if display:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "        axes[0, 0].imshow(rgb_img)\n",
    "        axes[1, 0].imshow(depth_img)\n",
    "        \n",
    "        axes[0, 0].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "        axes[1, 0].add_patch(get_bb_patch(pick_cord_dict, \"r\"))\n",
    "\n",
    "        axes[0, 0].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1, 0].text(pick_cord_dict[\"xmin\"], pick_cord_dict[\"ymin\"] - 10, f\"pick: {pick_cord_dict['strlabel']}\", color='r', fontsize=12, ha='left', va='bottom')\n",
    "\n",
    "        axes[0, 0].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "        axes[1, 0].add_patch(get_bb_patch(place_cord_dict, \"g\"))\n",
    "\n",
    "        axes[0, 0].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        axes[1, 0].text(place_cord_dict[\"xmin\"], place_cord_dict[\"ymin\"] - 10, f\"place {place_cord_dict['strlabel']}\", color='g', fontsize=12, ha='left', va='bottom')\n",
    "        \n",
    "        axes[0, 0].set_title(\"pick and place bb in rgb\", fontsize=14)\n",
    "        axes[1, 0].set_title(\"pick and place bb in depth\", fontsize=14)\n",
    "\n",
    "        sam_predictor.set_image(rgb_img)\n",
    "        \n",
    "        input_box = np.array([pick_cord_dict[\"xmin\"],  pick_cord_dict[\"ymin\"],  pick_cord_dict[\"xmax\"],  pick_cord_dict[\"ymax\"]])\n",
    "        pick_mask, pick_scores, pick_logits = sam_predictor.predict(box=input_box)\n",
    "        pick_mask = np.transpose(pick_mask, (1, 2, 0))\n",
    "        axes[0, 1].set_title(\"pick segmentation\", fontsize=14)\n",
    "        axes[0, 1].imshow(pick_mask)\n",
    "\n",
    "        input_box = np.array([place_cord_dict[\"xmin\"],  place_cord_dict[\"ymin\"],  place_cord_dict[\"xmax\"],  place_cord_dict[\"ymax\"]])\n",
    "        place_mask, place_scores, place_logits = sam_predictor.predict(box=input_box)\n",
    "        place_mask = np.transpose(place_mask, (1, 2, 0))\n",
    "        axes[1, 1].set_title(\"place segmentation\", fontsize=14)\n",
    "        axes[1, 1].imshow(place_mask)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    _, K = get_depth_frame_intrinsics(rs_wrapper)\n",
    "    depth_querry_list = [\n",
    "        (pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"]),\n",
    "        (pick_cord_dict['xmin'], pick_cord_dict['ymin']),\n",
    "        (pick_cord_dict['xmax'], pick_cord_dict['ymin']),\n",
    "        (place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"]),\n",
    "        (place_cord_dict['xmin'], place_cord_dict['ymin']),\n",
    "        (place_cord_dict['xmax'], place_cord_dict['ymin'])\n",
    "    ]\n",
    "    pick_c_d, pick_ll_d, pick_lr_d, place_c_d, place_ll_d, place_lr_d = get_topview_depth(depth_querry_list, UR_Interface, rs_wrapper)\n",
    "\n",
    "    pick_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, pick_cord_dict[\"xCenter\"], pick_cord_dict[\"yCenter\"], pick_c_d)\n",
    "    pick_LL_X, pick_LL_Y, pick_LL_Z = deproject_top_view_point(K, pick_cord_dict['xmin'], pick_cord_dict['ymin'], pick_ll_d)\n",
    "    pick_LR_X, pick_LR_Y, pick_LR_Z = deproject_top_view_point(K, pick_cord_dict['xmax'], pick_cord_dict['ymin'], pick_lr_d)\n",
    "    pick_sidelength =  (pick_LL_X-pick_LR_X)**2\n",
    "    pick_sidelength += (pick_LL_Y-pick_LR_Y)**2\n",
    "    pick_sidelength += (pick_LL_Z-pick_LR_Z)**2\n",
    "    pick_sidelength = np.sqrt(pick_sidelength)\n",
    "    pick_cord_dict[\"sidelength\"] = pick_sidelength\n",
    "    print(f\"{pick_cord_dict=}\")\n",
    "\n",
    "    place_cord_dict[\"ImgFrameWorldCoord\"] = deproject_top_view_point(K, place_cord_dict[\"xCenter\"], place_cord_dict[\"yCenter\"], place_c_d)\n",
    "    place_LL_X, place_LL_Y, place_LL_Z = deproject_top_view_point(K, place_cord_dict['xmin'], place_cord_dict['ymin'], place_ll_d)\n",
    "    place_LR_X, place_LR_Y, place_LR_Z = deproject_top_view_point(K, place_cord_dict['xmax'], place_cord_dict['ymin'], place_lr_d)\n",
    "    place_sidelength =  (place_LL_X-place_LR_X)**2\n",
    "    place_sidelength += (place_LL_Y-place_LR_Y)**2\n",
    "    place_sidelength += (place_LL_Z-place_LR_Z)**2\n",
    "    place_sidelength = np.sqrt(place_sidelength)\n",
    "    place_cord_dict[\"sidelength\"] = place_sidelength\n",
    "    print(f\"{place_cord_dict=}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #return dictionarys with {xmin, ymin, xmax, ymax, strlabel}\n",
    "    #return (pick_bbox, pick_cord_dict) , (place_bbox, place_cord_dict)\n",
    "    return pick_cord_dict, place_cord_dict\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset - bb[\"sidelength\"]\n",
    "    \n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    #print(goal_vec)\n",
    "    #UR_interface.disconnect()\n",
    "    #input()\n",
    "\n",
    "    UR_interface.close_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    return success\n",
    "\n",
    "#pick(myrobot, myrs, pick_bb, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place(UR_interface, bb):\n",
    "    X, Y, Z = bb[\"ImgFrameWorldCoord\"]\n",
    "    print(f\"Camera Frame Coordinates of Block: {X=}, {Y=}, {Z=}\\n\")\n",
    "    new_X = topview_vec[0] - Y + tcp_X_offset\n",
    "    new_Y = topview_vec[1] - X + tcp_Y_offset\n",
    "    new_Z = topview_vec[2] - Z + tcp_Z_offset\n",
    "\n",
    "    success = True\n",
    "    goal_vec = topview_vec.copy()\n",
    "    goal_vec[0] = new_X\n",
    "    goal_vec[1] = new_Y\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = new_Z\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "\n",
    "    UR_interface.open_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#myrobot.stop()\n",
    "#myrs.disconnect()\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\", \"orange block\"]\n",
    "tower = [ \"yellow block\", \"blue block\", \"green block\"]#, \"green block\"]\n",
    "#tower = [\"yellow block\", \"red block\"]\n",
    "for i in range(0, len(tower)-1):\n",
    "     pick_str = tower[i+1]\n",
    "     place_str = tower[i]\n",
    "     pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "     pick(myrobot, pick_bb)\n",
    "     place(myrobot, place_bb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_directory(base_dir):\n",
    "    # List all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    # Extract the numeric part of the directory names (e.g., 'run_0', 'run_1')\n",
    "    subdirectories = [d for d in subdirectories if d.startswith('run_')]\n",
    "    subdirectories = [int(d.split('_')[1]) for d in subdirectories if d.split('_')[1].isdigit()]\n",
    "    \n",
    "    # If there are no subdirectories, start from 0, else find the max and add 1\n",
    "    next_dir = max(subdirectories, default=-1) + 1\n",
    "\n",
    "    # Create the new directory with the name 'run_<next_dir>'\n",
    "    next_dir_path = os.path.join(base_dir, f\"run_{next_dir}\")\n",
    "    os.makedirs(next_dir_path, exist_ok=True)\n",
    "    \n",
    "    return next_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\n",
    "tower = [\"green block\", \"blue block\", \"yellow block\"]\n",
    "n_attempts = 2*len(tower)\n",
    "Done = 0\n",
    "i = 0\n",
    "save_dir = create_next_directory(\"./data_collection/\")\n",
    "\n",
    "while(not Done and i < n_attempts):\n",
    "    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\n",
    "    os.makedirs(interation_output_dir, exist_ok=True)\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"side view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\n",
    "    (state_resp, state), (instruction_resp, instruction) = get_gpt_next_instruction(client, rgb_img, tower)\n",
    "    Done = int(instruction[\"Done\"])\n",
    "    \n",
    "    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\n",
    "        print()\n",
    "        print(f\"{state['objects']=}\")\n",
    "        file.write(f\"{state['objects']=}\\n\")\n",
    "\n",
    "        print(f\"{state['object_relationships']=}\")\n",
    "        file.write(f\"{state['object_relationships']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['pick']=}\")\n",
    "        file.write(f\"{instruction['pick']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['place']=}\")\n",
    "        file.write(f\"{instruction['place']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['Done']=}\")\n",
    "        file.write(f\"{instruction['Done']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['explanation']=}\")\n",
    "        file.write(f\"{instruction['explanation']=}\\n\")\n",
    "\n",
    "        print(f\"{instruction['next_step']=}\")\n",
    "        file.write(f\"{instruction['next_step']=}\")\n",
    "\n",
    "        print()\n",
    "        file.write(f\"{state_resp=}\")\n",
    "        file.write(f\"{instruction_resp=}\")\n",
    "\n",
    "    if Done:\n",
    "        break\n",
    "    pick_str= instruction['pick']\n",
    "    place_str= instruction['place']\n",
    "    goto_vec(myrobot, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"top view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\n",
    "    pick_bb, place_bb = topview_pick_place_BB(myrobot, myrs, pick_str, place_str, display=True)\n",
    "    pick(myrobot, pick_bb)\n",
    "    place(myrobot, place_bb)\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
