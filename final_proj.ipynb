{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/sam2/sam2/modeling/sam/transformer.py:23: UserWarning: Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.\n",
      "  OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 18:35:07.263897: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-20 18:35:07.263923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-20 18:35:07.264897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 18:35:07.270250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-20 18:35:07.951390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#print(dir(sam2))\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "#from sam2.build_sam import build_sam2\n",
    "#from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "#from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.patches as patches\n",
    "#import sys\n",
    "from config import topview_vec, sideview_vec, vit_thresh, tower, realSenseFPS #, tcp_X_offset, tcp_Y_offset, tcp_Z_offset,\n",
    "# sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from magpie_control import realsense_wrapper as real\n",
    "from magpie_control.ur5 import UR5_Interface as robot\n",
    "from magpie_perception.label_owlv2 import LabelOWLv2\n",
    "from Observation import observation\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "#import random\n",
    "from openai import OpenAI\n",
    "\n",
    "from control_scripts import goto_vec, get_pictures, get_frames\n",
    "from gpt_planning import get_gpt_next_instruction, print_json\n",
    "from APIKeys import API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting robot from nb\n",
      "Found Dynamixel Port:\n",
      "/dev/ttyACM0\n",
      "\n",
      "Succeeded to open the port\n",
      "Succeeded to change the baudrate\n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n",
      "label_vit.model.device=device(type='cuda', index=0)\n",
      "sam_predictor.model.device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "myrs = real.RealSense(fps=realSenseFPS)\n",
    "myrs.initConnection()\n",
    "myrobot = robot()\n",
    "print(f\"starting robot from nb\")\n",
    "myrobot.start()\n",
    "label_vit = LabelOWLv2(topk=1, score_threshold=vit_thresh, cpu_override=False)\n",
    "label_vit.model.eval()\n",
    "#print(dir(label_vit.model))\n",
    "print(f\"{label_vit.model.device=}\")\n",
    "\n",
    "sam_predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "print(f\"{sam_predictor.model.device=}\")\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key= API_KEY,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n"
     ]
    }
   ],
   "source": [
    "myrobot.open_gripper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topview_pick_place_Observations(UR_Interface, rs_wrapper, pick_str, place_str, display= False):\n",
    "    #print(place_str)\n",
    "    #initlize local variables\n",
    "    goto_vec(UR_Interface, topview_vec)\n",
    "    pick_obs, place_obs = observation(str_label = pick_str) , observation(str_label=place_str)\n",
    "    transform = UR_Interface.get_tcp_pose()\n",
    "    print(f\"transform = {transform}\")\n",
    "    pick_obs.update_observation(rs_wrapper, label_vit, sam_predictor,  transform)\n",
    "    place_obs.update_observation(rs_wrapper, label_vit, sam_predictor,  transform)\n",
    "    return pick_obs, place_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(UR_interface, obs):\n",
    "\n",
    "    success = True\n",
    "    goal_vec = obs.pickPose.copy()\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    goal_vec[3] = topview_vec[3]\n",
    "    goal_vec[4] = topview_vec[4]\n",
    "    goal_vec[5] = topview_vec[5]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = obs.pickPose[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    UR_interface.close_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "    return success\n",
    "\n",
    "#pick(myrobot, myrs, pick_bb, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place(UR_interface, obs):\n",
    "\n",
    "    success = True\n",
    "    goal_vec = obs.placePose.copy()\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    goal_vec[3] = topview_vec[3]\n",
    "    goal_vec[4] = topview_vec[4]\n",
    "    goal_vec[5] = topview_vec[5]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    goal_vec[2] = obs.placePose[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "\n",
    "    UR_interface.open_gripper()\n",
    "    time.sleep(2)\n",
    "\n",
    "    goal_vec[2] = topview_vec[2]\n",
    "    success = goto_vec(UR_interface, goal_vec)\n",
    "\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform = [[-1.000e+00 -3.835e-05 -8.344e-06 -1.620e-01]\n",
      " [-3.835e-05  1.000e+00  4.104e-05 -6.235e-01]\n",
      " [ 8.342e-06  4.104e-05 -1.000e+00  5.474e-01]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  1.000e+00]]\n",
      "pose2world_transform.shape=(4, 4)\n",
      "offset_matrix.shape=(4, 4)\n",
      "pose2world_transform.shape=(4, 4)\n",
      "offset_matrix.shape=(4, 4)\n",
      "Position of dxl ID: 1 set to 586 \n",
      "Position of dxl ID: 2 set to 450 \n",
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n",
      "transform = [[-1.000e+00 -7.420e-05 -3.705e-05 -1.620e-01]\n",
      " [-7.420e-05  1.000e+00 -5.119e-05 -6.235e-01]\n",
      " [ 3.706e-05 -5.118e-05 -1.000e+00  5.475e-01]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  1.000e+00]]\n",
      "pose2world_transform.shape=(4, 4)\n",
      "offset_matrix.shape=(4, 4)\n",
      "pose2world_transform.shape=(4, 4)\n",
      "offset_matrix.shape=(4, 4)\n",
      "Position of dxl ID: 1 set to 586 \n",
      "Position of dxl ID: 2 set to 450 \n",
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#myrobot.stop()\n",
    "#myrs.disconnect()\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\", \"orange block\"]\n",
    "temp_tower = [ \"yellow block\", \"blue block\", \"green block\"]#, \"green block\"]\n",
    "#tower = [\"yellow block\", \"red block\"]\n",
    "for i in range(0, len(temp_tower)-1):\n",
    "     pick_str = temp_tower[i+1]\n",
    "     place_str = temp_tower[i]\n",
    "     pick_obs, place_obs = topview_pick_place_Observations(myrobot, myrs, pick_str, place_str, display=False)\n",
    "     pick(myrobot, pick_obs)\n",
    "     place(myrobot, place_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES LLM PLANNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_directory(base_dir):\n",
    "    # List all subdirectories in the base directory\n",
    "    subdirectories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    # Extract the numeric part of the directory names (e.g., 'run_0', 'run_1')\n",
    "    subdirectories = [d for d in subdirectories if d.startswith('run_')]\n",
    "    subdirectories = [int(d.split('_')[1]) for d in subdirectories if d.split('_')[1].isdigit()]\n",
    "    \n",
    "    # If there are no subdirectories, start from 0, else find the max and add 1\n",
    "    next_dir = max(subdirectories, default=-1) + 1\n",
    "\n",
    "    # Create the new directory with the name 'run_<next_dir>'\n",
    "    next_dir_path = os.path.join(base_dir, f\"run_{next_dir}\")\n",
    "    os.makedirs(next_dir_path, exist_ok=True)\n",
    "    \n",
    "    return next_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\\n#tower = tower.reverse()\\nn_attempts = 2*len(tower)\\nDone = 0\\ni = 0\\ntop_dir = \"./data_collection/\"\\nos.makedirs(top_dir, exist_ok=True)\\nsave_dir = create_next_directory(top_dir)\\naction_history = []\\nprevious_plan = []\\nwhile(not Done and i < n_attempts):\\n    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\\n    os.makedirs(interation_output_dir, exist_ok=True)\\n    goto_vec(myrobot, sideview_vec)\\n    rgb_img, depth_img = get_pictures(myrs)\\n    plt.imshow(rgb_img)\\n    plt.title(f\"side view {i}\")\\n    plt.show()\\n    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\\n\\n    (state_resp, state, state_sys_prompt, state_usr_promot), (instruction_resp, next_instruction, future_instructions, instruction_sys_prompt, instruction_usr_prompt) = get_gpt_next_instruction(client, rgb_img, tower, action_history, previous_plan)\\n    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\\n        state_str = print_json(state, name=\"state\")\\n        file.write(f\"{state_str}\\n\")\\n\\n        next_instruction_str = print_json(next_instruction, \"next instruction\")\\n        file.write(f\"{next_instruction_str}\")\\n\\n\\n        instruction_plan_str = print_json(future_instructions, name=\"plan\")\\n        file.write(f\"{instruction_plan_str}\")\\n\\n        file.write(f\"{state_sys_prompt=}\\n\")\\n        file.write(f\"{state_usr_promot=}\\n\")\\n        file.write(f\"{instruction_sys_prompt=}\\n\")\\n        file.write(f\"{instruction_usr_prompt=}\\n\")\\n        print(f\"{instruction_usr_prompt=}\")\\n    \\n    action_history.append(next_instruction)\\n    previous_plan = future_instructions\\n    Done = int(next_instruction[\"done\"])\\n    \\n   \\n\\n    if Done:\\n        break\\n    pick_str= next_instruction[\\'pick\\']\\n    place_str= next_instruction[\\'place\\']\\n    goto_vec(myrobot, topview_vec)\\n    rgb_img, depth_img = get_pictures(myrs)\\n    plt.imshow(rgb_img)\\n    plt.title(f\"top view {i}\")\\n    plt.show()\\n    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\\n    pick_obs, place_obs = topview_pick_place_Observations(myrobot, myrs, pick_str, place_str, display=True)\\n    pick(myrobot, pick_obs)\\n    place(myrobot, place_obs)\\n    i += 1\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#tower = [\"red block\", \"blue block\", \"green block\", \"yellow block\", \"purple block\"]\n",
    "#tower = tower.reverse()\n",
    "n_attempts = 2*len(tower)\n",
    "Done = 0\n",
    "i = 0\n",
    "top_dir = \"./data_collection/\"\n",
    "os.makedirs(top_dir, exist_ok=True)\n",
    "save_dir = create_next_directory(top_dir)\n",
    "action_history = []\n",
    "previous_plan = []\n",
    "while(not Done and i < n_attempts):\n",
    "    interation_output_dir = os.path.join(save_dir, f\"step {i}\")\n",
    "    os.makedirs(interation_output_dir, exist_ok=True)\n",
    "    goto_vec(myrobot, sideview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"side view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"sideview.png\"), rgb_img)\n",
    "\n",
    "    (state_resp, state, state_sys_prompt, state_usr_promot), (instruction_resp, next_instruction, future_instructions, instruction_sys_prompt, instruction_usr_prompt) = get_gpt_next_instruction(client, rgb_img, tower, action_history, previous_plan)\n",
    "    with open(os.path.join(interation_output_dir, \"instruction.txt\"), \"w\") as file:\n",
    "        state_str = print_json(state, name=\"state\")\n",
    "        file.write(f\"{state_str}\\n\")\n",
    "\n",
    "        next_instruction_str = print_json(next_instruction, \"next instruction\")\n",
    "        file.write(f\"{next_instruction_str}\")\n",
    "\n",
    "\n",
    "        instruction_plan_str = print_json(future_instructions, name=\"plan\")\n",
    "        file.write(f\"{instruction_plan_str}\")\n",
    "\n",
    "        file.write(f\"{state_sys_prompt=}\\n\")\n",
    "        file.write(f\"{state_usr_promot=}\\n\")\n",
    "        file.write(f\"{instruction_sys_prompt=}\\n\")\n",
    "        file.write(f\"{instruction_usr_prompt=}\\n\")\n",
    "        print(f\"{instruction_usr_prompt=}\")\n",
    "    \n",
    "    action_history.append(next_instruction)\n",
    "    previous_plan = future_instructions\n",
    "    Done = int(next_instruction[\"done\"])\n",
    "    \n",
    "   \n",
    "\n",
    "    if Done:\n",
    "        break\n",
    "    pick_str= next_instruction['pick']\n",
    "    place_str= next_instruction['place']\n",
    "    goto_vec(myrobot, topview_vec)\n",
    "    rgb_img, depth_img = get_pictures(myrs)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(f\"top view {i}\")\n",
    "    plt.show()\n",
    "    plt.imsave(os.path.join(interation_output_dir, \"topview.png\"), rgb_img)\n",
    "    pick_obs, place_obs = topview_pick_place_Observations(myrobot, myrs, pick_str, place_str, display=True)\n",
    "    pick(myrobot, pick_obs)\n",
    "    place(myrobot, place_obs)\n",
    "    i += 1\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
